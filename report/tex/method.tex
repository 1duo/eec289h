%Describe in detail the feature representation(s) and algorithm(s) you employed.
%The description should be self-contained (i.e., the reader should not have to
%rely on outside sources for your points to be clear), and should provide enough
%detail so that the reader could re-implement the approach. Clearly state the
%method's input and output, and any assumptions or design choices;

Previous work on Convolutional Neural Networks (CNNs) implies that it may capture the high-level representation of an image
using a certain deep layer feature set. Our goal of this project is to answer one single question: \emph{Whether CNNs can help with the feature representation
to extract high-level inforamtion of an image scene and thus improve the scene classification precision?} 
We choose a CNN which is pre-trained on ImageNet dataset (ImageNet-CNN) since its a large-scale general object recognition datasets which consists of over 15 million labeled
high-resolution images in over 22,000 categories. We use CNN pretrained on such dataset with the hope to reduce the chance of overfitting to certain scenes.
To utilize a pre-trained ImageNet CNN and for the efficiency of the feature extraction process, we use a popular library: Caffe~\cite{Jia:2014:CCA}.

For the training process, our system takes all images in the training set for each category as the input, use the ImageNet-CNN to perform a prediction for each image. Instead of
getting the final 1000 length prediction vector, we extract the FC 7 layer feature set which contains 4096 response values. We then use such features to train one linear SVM model
for each scene category. For the testing process, an input image goes through the same ImageNet-CNN and its 4096 length deep feature vector are used to predict its scene classification
for each linear SVM model and we assign the one with highest confidence score.

%I think one good way to write a paper is to leave the climax to the very last part. Or keep it coming little by little. You give it away too soon here. It's like making love and finishing in
%3 minutes and now all the girl wants to do is getting rid of you and taking some fresh air to cool her foolish mind. Anyway, I suggest we go slow, we list our improvements one by one.
%I will do it tomorrow morning. Lemme show you how to make a girl come!

extract around 2000 bottom-up region
proposals using selective search\cite{Uijlings:2013:SSO}, then extract
4096-dimensional feature vectors for each region proposal using a large
convolutional neural network (CNN) library Caffe\cite{Jia:2014:Caffe},
after we got feature for each region proposal, we apply spatial pyramid and
do max-pooling to find the features that contribute most. We then perform a
L2 normalization procedure for all feature matrices. The final feature matrices
then used for classification using multi-class linear SVMs classifier. Our
method achieves a mean average precision (mAP) of 68.2953\% on dataset
MIT-indoor67\cite{Quattoni:2009:RIS} without fine-tune on this dataset.
For comparison, we implement using only 4096-dimensional feature vectors
extracted from Caffe without region proposals, spatial pyramid matching 
and max-pooling which has the mAP of 59.9507\%.

\begin{figure*}[ht]
  \centering
  \includegraphics[scale=0.8]{img/demo.pdf}
  \centering
  \caption{The overview of our system. For an input image, a selective search algorithm is applied first to get roughly 2000 regions of interest. We then apply a pre-trained Convolutional Neural Network (CNN) on each region of interest to get a deep feature vector of length 4096. A three-level spatial pyramid representation of the image with deep feature is used to create the final feature representation. At each level, for each spatial bin, we use max pooling to get the largest feature value of all the feature values of the regions of interest which fall into that spatial bin, resulting in the final feature of length No.Deep Features $\times$ (1+4+16) as a high-level representation of the input image. Then multiple one-vs-all linear SVMs are used to do the scene classification.}
  \label{fig:system_overview}
\end{figure*}

\subsection{Selective Search}
A variety of recent research offers methods for generating category-independent
region proposals for possible object locations.
Selective search is widely used for generating possible object locations for
use in object recognition\cite{Uijlings:2013:SSO}. Same strategies can be
adopted on indoor scene classification. For the indoor scenes that can be well
characterized by objects they contain, the selective search can exploit local
discriminative information with greatly reduced number of locations compared
to an exhaustive search. We use selective search to generate region proposals.
Caffe provides a general Python interface for models and it has built in
interface for selective search. We only need to change the setting of CROP\_MODES
to selective\_search, we can operate on around 2000 region proposals instead of
the entire image.

\subsection{Feature Extraction}
Caffe~\cite{Jia:2014:CCA} is an open source convolutional architecture for
fast feature embedding which contains pre-trained models.
In our project, we use pre-trained BVLC Reference CaffeNet to classify images
and extract 4096-dimensional layer 7 feature vectors from each region
proposal using Caffe~\cite{Jia:2014:CCA} implementation of the CNN described by
Krizhevsky et al\cite{Krizhevsky:2012:ICD}. It provides an option to output the
features in certain layer rather than only the final classification results. We
set the \textit{blobs} option to \textit{fc7} in order to obtained the Layer
7 feature vectors for each of the region proposals. Therefore, after this step,
for one input image, we obtain around 2000 feature vectors and each have the
dimension of 4096.

\subsection{Max Pooling}

\subsection{Spatial Pyramid Matching}
For each image, a three-level spatial pyramid representation is used, resulting
$numImages * numWindows * (1 + 4 + 16)$ length feature vectors.

\subsection{L2 Normalization}
We then perform the L2 normalizations for each of the feature matrix.
L2 normalization is computed by the square root of the sum of each feature's
square and for each feature in the feature vector. By dividing each feature
vector the L2 normalization value, we have each feature vector's
L2 normalization = 1. This L2 normalizations have only a modest effect on the
overall performance perhaps simply because the original feature vector data
already in the similar scale.

\subsection{Training}
SVMs (Support Vector Machines) are a useful technique for data classification.
LibSVM\cite{Chang:2011:CC01a} is an integrated software for support vector
classification that is widely used in variety of classification tasks.
It supports multi-class classification which is used in this project.
We trained 67 binary one-vs-all SVM classifiers each for one category in
MIT-indoor67 dataset.
In order to fit the required LibSVM training file format, for each category
training file, we add label +1 to each feature vector that belongs to the
category and label -1 to all feature vectors that belong to rest of categories.
We also move all instances that belonging to the current positive category
(+1 labeled feature vectors) to be at the top of the feature matrix, this would
guarantee the correctness even if LibSVM might internally map the label of the
first training instance to be +1 regardless of its actual label value.
