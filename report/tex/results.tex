%Describe the experiments you conducted to evaluate the approach.  For each
%experiment, describe what you did, what was the main purpose of the experiment,
%and what you learned from the results. Provide figures, tables, and qualitative
%examples, as appropriate.

In this section, we evaluate our method on the MIT-indoor67 dataset. Suggested training and
testing list of images are used to do the training (80 images per class) and validation (20
images per class), all images are in jpeg format.

Multi-class classification is done with a 67 SVMs trained using one-versus-all rule, that is, each
classifier is learned to separate each class from the rest of classes. Test image is assigned the
label of the class with the highest confidence score. Scene classification performance is
evaluated by the average multi-class classification accuracy over all scene classes.

For comparison purpose, we implement with the same procedure but only use
the extracted layer 7 4096-dimensional feature vectors from Caffe. After we get
one feature vectors for each entire image, instead of perform spatial pyramid
and L2 normalization, we simply add labels and send them into the multi-class
SVMs. Validation image feature vectors are also generated in the same way.

% Table 1
\begin{table*}[ht]
        \caption{Comparison results on MIT-indoor67}
        \centering
        \begin{tabular}{l c c}
        \hline \hline
        Models                & Average Precision \\ \hline
        $l2$ Normlization + Selective Search + Spatial Pyramid & {\bf{68.2953\%}} \\
        Selective Search + Spatial Pyramid & 68.0469\% \\
        Entire Image CNN Features & 59.9507\% \\
        \hline
        \end{tabular}
        \label{tab:overall}
\end{table*}

\subsection{Quantitative Evaluation}
We compare our scene classification performance with two other methods: 1) using only the
features extracted from the entire image; 2) using selective search and spatial pyramid, but without the
$l2$ normalization. The summary of our performance comparison is listed in Table~\ref{tab:overall}.
Our method achieves a mean average precision (mAP) of 68.2953\% on dataset
MIT-indoor67. For comparison, we implement the same method using only 4096-dimensional
feature vectors extracted from Caffe without region proposals, spatial pyramid, and max-pooling.
Using selective search and spatial pyramid gives us a 8.1\% performance gain and introducing
the $l2$ normalization gives us an extra 0.25\% performance gain. In most categories, we perform much
better than the average precision. Some examples are shoes shop, bedroom, grocery store, hospital room and operating
room. We achieve 100\% precision on three categories: cloister, florist, and bowling. This is because the region proposals and spatial pyramid technique
allow us to better characterize the particular objects belong to the category. For those categories which achieve worst precision, the false positives are
not evenly distributed either, but are focused on some very sensible categories. For example the top two false positive categories for auditorium are concert hall
and movie theater.
We also note some drops of average accuracy using our methods. The drops mainly happen for on the following three categories: prison-cell, library and living room.
Note these three categories are all relatively easier to be characterized by global spatial properties (prison cell bars and books on shelves) so focusing on small regions
might suppress the representation of the global scene.

% top 5 best and top 5 worst table (Table 2)
\begin{table}[ht]
        \caption{Top 5 Best and Top 5 Worst Results}
        \centering
        \begin{tabular}{l l l}
        \hline \hline
        & Name           & Avg. Prec./Top FP Ctgr. \\ \hline
        \multirow{5}{*}{Top 5 Best}
        & cloister       & 100\% \\
        & florist        & 100\% \\
        & bowling        & 100\% \\
        & poolinside     & 95\% \\
        & greenhouse     & 94.74\% \\
        \hline
        \multirow{5}{*}{Top 5 Worst}
        & livingroom     & 20\%/bedroom \\
        & lobby          & 30\%/jelleryshop \\
        & deli           & 31.58\%/bakery \\
        & office         & 33.33\%/computer\_room \\
        & airport inside & 35\%/subway \\
        \hline
        \end{tabular}
        \label{tab:overall}
\end{table}

Table~\ref{tab:compare} compares the performance of our method against various other scene classification methods. Note that methods using CNNs all show significant improvements of the performance.
Also, our method which uses selective search and spatial pyramid achieves better performance than Zhou et al.'s work on both ImageNet-CNN and Places-CNN, a CNN trained on a dataset specific created for places.

% compare with other paper results
% Table 3
\begin{table}[ht]
        \caption{Comparison to other methods}
        \centering
        \begin{tabular}{l c c}
        \hline \hline
        Method                & Average Precision \\ \hline
        Object Bank~\protect\cite{Li:2010:OBA} & 37.6 \\
        DPM+GIST-color+SP~\protect\cite{Pandey:2011:SRW} & 43.1 \\
        ImageNet-CNN feature~\protect\cite{Zhou:2014:LDF} & 56.79 \\
        Our Method & 68.3 \\
        Places-CNN feature~\protect\cite{Zhou:2014:LDF} & 68.24 \\
        \hline
        \end{tabular}
        \label{tab:compare}
\end{table}

% heatmap visulizations
\subsection{Qualitative Evaluation}
